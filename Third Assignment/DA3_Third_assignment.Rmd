---
title: "DA3_Third_Assignment"
author: "Akos Almasi"
date: "2023-02-23"
output: html_document
---

```{r}
# CLEAR MEMORY
rm(list=ls())
# Libraries used for the project
library(tidyverse)
library(haven)
library(glmnet)
library(purrr)
library(margins)
library(skimr)
library(kableExtra)
library(Hmisc)
library(cowplot)
library(gmodels) 
library(lspline)
library(sandwich)
library(modelsummary)

library(rattle)
library(caret)
library(pROC)
library(ranger)
library(rpart)
library(rpart.plot)
library(fixest)

source(paste(dirname(rstudioapi::getSourceEditorContext()$path), 'utils.R', sep = '/'))

```

### Data import
```{r}
# Read the data
data <- read.csv('https://raw.githubusercontent.com/akos-almasi/Data-Analysis-3/main/Third%20Assignment/cs_bisnode_panel.csv')
```

```{r}
# Check our data
glimpse(data)

# Descriptive statistics
skimr::skim(data)
```

### Label engineering
```{r}
# Number of firms
length(unique(data$comp_id))
# drop variables with many NA's
data <- data %>%
  select(-c(COGS, finished_prod, net_dom_sales, net_exp_sales, wages))

# Filter years, from 2010 to 2015
data <- data %>% filter(year %in% c(2012, 2013, 2014))

# add all year and comp_id combinations 
data <- data %>%
  complete(year, comp_id)

# generate status_alive; if sales larger than zero and not-NA, then firm is alive
data  <- data %>%
  mutate(status_alive = sales > 0 & !is.na(sales) %>%
           as.numeric(.))


summary(data$sales)


data <- data %>%
  mutate(sales = ifelse(sales < 0, 1, sales),
         ln_sales = ifelse(sales > 0, log(sales), 0),
         sales_mil=sales/1000000,
         sales_mil_log = ifelse(sales > 0, log(sales_mil), 0))

data <- data %>%
  group_by(comp_id) %>%
  mutate(d1_sales_mil_log = sales_mil_log - Lag(sales_mil_log, 1) ) %>%
  ungroup()


# replace w 0 for new firms + add dummy to capture it
data <- data %>%
  mutate(age = (year - founded_year) %>%
           ifelse(. < 0, 0, .),
         new = as.numeric(age <= 1) %>% #  (age could be 0,1 )
           ifelse(balsheet_notfullyear == 1, 1, .),
         d1_sales_mil_log = ifelse(new == 1, 0, d1_sales_mil_log),
         new = ifelse(is.na(d1_sales_mil_log), 1, new),
         d1_sales_mil_log = ifelse(is.na(d1_sales_mil_log), 0, d1_sales_mil_log))

# Look at only firms who has a status alive value 1
# look at cross section
data <- data %>%
  filter (status_alive == 1)

P01 <- function(x){quantile(x, 0.01)}
P99 <- function(x){quantile(x, 0.99)}
datasummary( sales_mil~ P0 + P01 + P25 + P50 + P75 +  P99 + P100 + Mean + SD,  
             data = data , output = 'markdown')

data <- data %>%
  # look at firms below 11.13m euro and above 1000 euros revenues
  filter(!(sales_mil > 11.13)) %>%
  filter(!(sales_mil < 0.001))
```
### Feature engineering
```{r}
# change some industry category codes
data <- data %>%
  mutate(ind2_cat = ind2 %>%
           ifelse(. > 56, 60, .)  %>%
           ifelse(. < 26, 20, .) %>%
           ifelse(. < 55 & . > 35, 40, .) %>%
           ifelse(. == 31, 30, .) %>%
           ifelse(is.na(.), 99, .)
           )

table(data$ind2_cat)

# Firm characteristics
data <- data %>%
  mutate(age2 = age^2,
         foreign_management = as.numeric(foreign >= 0.5),
         gender_m = factor(gender, levels = c("female", "male", "mix")),
         m_region_loc = factor(region_m, levels = c("Central", "East", "West")))

# assets can't be negative. Change them to 0 and add a flag.
data <-data  %>%
  mutate(flag_asset_problem=ifelse(intang_assets<0 | curr_assets<0 | fixed_assets<0,1,0  ))
table(data$flag_asset_problem)

data <- data %>%
  mutate(intang_assets = ifelse(intang_assets < 0, 0, intang_assets),
         curr_assets = ifelse(curr_assets < 0, 0, curr_assets),
         fixed_assets = ifelse(fixed_assets < 0, 0, fixed_assets))

# generate total assets
data <- data %>%
  mutate(total_assets_bs = intang_assets + curr_assets + fixed_assets)
summary(data$total_assets_bs)


pl_names <- c("extra_exp","extra_inc",  "extra_profit_loss", "inc_bef_tax" ,"inventories",
              "material_exp", "profit_loss_year", "personnel_exp")
bs_names <- c("intang_assets", "curr_liab", "fixed_assets", "liq_assets", "curr_assets",
              "share_eq", "subscribed_cap", "tang_assets" )

# divide all pl_names elements by sales and create new column for it
data <- data %>%
  mutate_at(vars(pl_names), funs("pl"=./sales))

# divide all bs_names elements by total_assets_bs and create new column for it
data <- data %>%
  mutate_at(vars(bs_names), funs("bs"=ifelse(total_assets_bs == 0, 0, ./total_assets_bs)))


# creating flags, and winsorizing tails
# Variables that represent accounting items that cannot be negative (e.g. materials)
zero <-  c("extra_exp_pl", "extra_inc_pl", "inventories_pl", "material_exp_pl", "personnel_exp_pl",
           "curr_liab_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs", "subscribed_cap_bs",
           "intang_assets_bs")

data <- data %>%
  mutate_at(vars(zero), funs("flag_high"= as.numeric(.> 1))) %>%
  mutate_at(vars(zero), funs(ifelse(.> 1, 1, .))) %>%
  mutate_at(vars(zero), funs("flag_error"= as.numeric(.< 0))) %>%
  mutate_at(vars(zero), funs(ifelse(.< 0, 0, .)))


# for vars that could be any, but are mostly between -1 and 1
any <-  c("extra_profit_loss_pl", "inc_bef_tax_pl", "profit_loss_year_pl", "share_eq_bs")

data <- data %>%
  mutate_at(vars(any), funs("flag_low"= as.numeric(.< -1))) %>%
  mutate_at(vars(any), funs(ifelse(.< -1, -1, .))) %>%
  mutate_at(vars(any), funs("flag_high"= as.numeric(.> 1))) %>%
  mutate_at(vars(any), funs(ifelse(.> 1, 1, .))) %>%
  mutate_at(vars(any), funs("flag_zero"= as.numeric(.== 0))) %>%
  mutate_at(vars(any), funs("quad"= .^2))


# dropping flags with no variation
variances<- data %>%
  select(contains("flag")) %>%
  apply(2, var, na.rm = TRUE) == 0

data <- data %>%
  select(-one_of(names(variances)[variances]))


# additional
# CEO age
data <- data %>%
  mutate(ceo_age = year-birth_year,
         flag_low_ceo_age = as.numeric(ceo_age < 25 & !is.na(ceo_age)),
         flag_high_ceo_age = as.numeric(ceo_age > 75 & !is.na(ceo_age)),
         flag_miss_ceo_age = as.numeric(is.na(ceo_age)))

data <- data %>%
  mutate(ceo_age = ifelse(ceo_age < 25, 25, ceo_age) %>%
           ifelse(. > 75, 75, .) %>%
           ifelse(is.na(.), mean(., na.rm = TRUE), .),
         ceo_young = as.numeric(ceo_age < 40))

# number emp, very noisy measure
data <- data %>%
  mutate(labor_avg_mod = ifelse(is.na(labor_avg), mean(labor_avg, na.rm = TRUE), labor_avg),
         flag_miss_labor_avg = as.numeric(is.na(labor_avg)))

summary(data$labor_avg)
summary(data$labor_avg_mod)

data <- data %>%
  select(-labor_avg)

# create factors
data <- data %>%
  mutate(urban_m = factor(urban_m, levels = c(1,2,3)),
         ind2_cat = factor(ind2_cat, levels = sort(unique(data$ind2_cat))))

# Sales_mil squared column
data <- data %>%
  mutate(sales_mil_log_sq=sales_mil_log^2)

skimr::skim(data)
```
### Why I chose this term to define fast growing
Over a relatively short period of time, a threshold of 0.4 for sales may be appropriate. This would capture the companies, that experienced a 40% increase in sales over a two-year period, which could be indicative of a successful growth strategy. Companies growing at such a rate are likely to have significant opportunities for expansion and profitability. It provides a good balance between sensitivity and specificity, since it can identify a subset of companies that experienced substantial growth.

```{r}
# Define Target variable -> fast growing companies
data <- data %>%
  group_by(comp_id) %>%
  summarise(growth = ifelse((sales[year == 2014] / sales[year == 2012] - 1) > 0.4, 1, 0)) %>% 
  left_join(data, by= 'comp_id')

data <- data %>%
  mutate(growth_f = factor(growth, levels = c(0,1)) %>%
           recode(., `0` = 'no_growth', `1` = "growth"))

# Dealing with NA values
colSums(is.na(data))

# drop these observations if they are missing, since they are key variables
data <- data %>% filter(!is.na(amort))
data <- data %>% filter(!is.na(extra_exp))
data <- data %>% filter(!is.na(curr_assets))
data <- data %>% filter(!is.na(profit_loss_year))
data <- data %>% filter(!is.na(age))
data <- data %>% filter(!is.na(m_region_loc))
```

```{r}
# visual representation of the growth ratio
ggplot(data = data, aes(x = growth)) +
  geom_histogram(aes(y = ..count.. / sum(count)), 
                 binwidth = 0.5, 
                 fill = "skyblue4", 
                 color = "white", 
                 size = 0.1,
                 alpha = 0.8) +
  labs(x = "0: Stagnant, 1: Fast growing", 
       y = "Probabilities") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1), 
                     limits = c(0, 1)) + 
  theme_minimal() +
  theme( 
        axis.line = element_line(colour = "black"), 
        axis.text = element_text(size = 12), 
        axis.title = element_text(size = 14))
```
```{r}
# sales distribution of firms
ggplot(data = data, aes(x = sales_mil)) + 
  geom_histogram(
    aes(y = (..count..)/sum(..count..)),
    bins = 50, fill = 'skyblue4', col = 'white', size = 0.25, alpha = 0.8) + 
  theme_bw() + 
  labs(
    title = "The distribution of sales of firms", 
    x = "Sales (Million â‚¬)", y = "percent of total"
  ) +
  scale_y_continuous(
    expand = c(0.01,0.01),
    labels = scales::percent_format(accuracy = 0.1)
  ) +
  theme(plot.title = element_text(size = rel(1))) +
  scale_x_continuous(expand = c(0.01,0.01))

```



```{r}
colors <- c('original datapoints'= 'black', 'linear model'='#990033', 'loess'= 'blue')

ggplot(
  data = data, 
  aes(x=sales_mil_log)) +
  geom_point(aes( y=as.numeric(growth), color="original datapoints"),
    size=1,  shape=20, stroke=2, fill="black" ) +
  geom_smooth(aes(y=as.numeric(growth), color= 'linear model'), 
    method = "lm", formula = y ~ poly(x,2), se = F, size=1.5)+
  geom_smooth(aes(y=as.numeric(growth), color= 'loess'),
    method="loess", se=F, size=1.5, span=1) +
  labs(
    x = "sales_mil_log",y = "growth", 
    color = '', title = 'Sales vs growth probabilities') +
  scale_color_manual(values = colors) + 
  theme_bw()

```
### Model preparation
```{r}
###############################################################################################
# MODELLING PREP
###############################################################################################

# Define variable sets
rawvars <-  c("curr_assets", "curr_liab", "extra_exp", "extra_inc", 
              "extra_profit_loss", "fixed_assets","inc_bef_tax", 
              "intang_assets", "inventories", "liq_assets", "material_exp", 
              "personnel_exp","profit_loss_year", "share_eq",'sales', "subscribed_cap")

qualityvars <- c("balsheet_flag", "balsheet_length", "balsheet_notfullyear")

engvar <- c("total_assets_bs", "fixed_assets_bs", "liq_assets_bs", 
            "curr_assets_bs","share_eq_bs", "subscribed_cap_bs", 
            "intang_assets_bs", "extra_exp_pl","extra_inc_pl", 
            "extra_profit_loss_pl", "inc_bef_tax_pl", "inventories_pl",
            "material_exp_pl", "profit_loss_year_pl", "personnel_exp_pl")

engvar2 <- c("extra_profit_loss_pl_quad", "inc_bef_tax_pl_quad",
             "profit_loss_year_pl_quad", "share_eq_bs_quad")

engvar3 <- c(grep("*flag_low$", names(data), value = TRUE),
             grep("*flag_high$", names(data), value = TRUE),
             grep("*flag_error$", names(data), value = TRUE),
             grep("*flag_zero$", names(data), value = TRUE))

hr <- c("female", "ceo_age", "flag_high_ceo_age", "flag_low_ceo_age",
        "flag_miss_ceo_age", "ceo_count", "labor_avg_mod",
        "flag_miss_labor_avg", "foreign_management")

firm <- c("age", "age2", "new", "ind2_cat", "m_region_loc", "urban_m")

# Interactions for logit & lasso

interactions <- c("ind2_cat*age", "ind2_cat*age2",
                   "ind2_cat*ceo_age", "ind2_cat*foreign_management",
                   "ind2_cat*female",   "ind2_cat*urban_m", 
                   "ind2_cat*labor_avg_mod")

#####################
# Model Setup
####################

# Logit models

X1 <- c(rawvars, firm, engvar)

X2 <- c(rawvars, firm, engvar, engvar2, engvar3, hr, qualityvars)


# for LASSO
logitvars <- c(rawvars, engvar, engvar2, engvar3, hr, firm, qualityvars, interactions)

# for RF (no interactions, no modified features)
rfvars  <-  c( rawvars, hr, firm, qualityvars)

```


### Modelling
```{r}
######################
# STEP 0)
#   separate datasets

set.seed(13505)
# Create train and holdout samples
train_indices <- as.integer(createDataPartition(data$growth, p = 0.8, list = FALSE))
data_train    <- data[train_indices, ]
data_holdout  <- data[-train_indices, ]

dim(data_train)
dim(data_holdout)



#######################################################
# Step 1) Predict probabilities 
#   with logit + logit and LASSO models
#     using CV

# calculate metrics and store them in containers
logit_models <- list()
CV_RMSE_folds <- list()
CV_AUC_folds <- list()

# 5 fold cross-validation:
#   check the summary function
train_control <- trainControl(
  method = 'cv',
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)

####
# a) Cross-Validate Logit Models 
logit_model_vars <- list('X1' = X1, 'X2' = X2)


for (model_name in names(logit_model_vars)) {

  # setting the variables for each model
  features <- logit_model_vars[[model_name]]

  print(paste0('Model: ', model_name, ', number of features: ', length(features)))
  # Estimate logit model with 5-fold CV
  set.seed(13505)
  glm_model <- train(
    formula(paste0('growth_f ~', paste0(features, collapse = ' + '))),
    method    = 'glm',
    data      = data_train,
    family    = binomial,
    trControl = train_control
 )
  # Save the results to list
  logit_models[[model_name]] <- glm_model
  # Save RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c('Resample', 'RMSE')]
  print(paste0('CV RMSE: ', glm_model$resample$RMSE))
  print('  ')
}
 

#####
# b) Logit + LASSO

# Set lambda parameters to check
lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid('alpha' = 1, lambda = lambda)

# Estimate logit + LASSO with 5-fold CV to find lambda
set.seed(13505)
system.time({
  logit_lasso_model <- train(
    formula(paste0('growth_f ~', paste0(logitvars, collapse = ' + '))),
    data = data_train,
    method = 'glmnet',
    preProcess = c('center', 'scale'),
    family = 'binomial',
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
 )
})

# Save the results
tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[['LASSO']] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
CV_RMSE_folds[['LASSO']] <- logit_lasso_model$resample[,c('Resample', 'RMSE')]


# calculate AUC for each model for each fold

for (model_name in names(logit_models)) {
  
  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    # get the prediction from each fold
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    # calculate the roc curve
    roc_obj <- roc(cv_fold$obs, cv_fold$growth, quiet = TRUE)
    # save the AUC value
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }
  
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                           "AUC" = unlist(auc))
}

```

```{r}
# Step 2 CLASSIFICATION

# using logit model 'X2' for prediction metrics demo
best_logit_no_loss <- logit_models[['X2']]


logit_predicted_probabilities_holdout    <- predict(best_logit_no_loss, newdata = data_holdout, type = 'prob')
data_holdout[,'best_logit_no_loss_pred'] <- logit_predicted_probabilities_holdout[,'growth']
RMSE(data_holdout[, 'best_logit_no_loss_pred', drop=TRUE], data_holdout$default)

# 2.1 calibration plot: how did we do in predicting the probabilities?
create_calibration_plot(
  data_holdout, 
  prob_var = "best_logit_no_loss_pred", 
  actual_var = "growth",
  n_bins = 10)

# 2.2 confusion matrix
# With the threshold: mean of predicted probabilities

mean_predicted_default_prob <- mean(data_holdout$best_logit_no_loss_pred)
mean_predicted_default_prob
holdout_prediction <-
  ifelse(data_holdout$best_logit_no_loss_pred < mean_predicted_default_prob, "no_growth", "growth") %>%
  factor(levels = c("no_growth", "growth"))
cm_object_2 <- confusionMatrix(holdout_prediction,as.factor(data_holdout$growth_f))
cm_2 <- cm_object_2$table
cm_2


# 2.3 ROC curve

# a) generate ROC curve by hand using calculated TPR & FPR values

thresholds <- seq(0.05, 0.75, by = 0.05)

cm <- list()
true_positive_rates <- c()
false_positive_rates <- c()
for (thr in thresholds) {
  # get holdout prediction
  holdout_prediction <- ifelse(data_holdout[,"best_logit_no_loss_pred"] < thr, "no_growth", "growth") %>%
    factor(levels = c("no_growth", "growth"))
  # create confusion matrix
  cm_thr <- confusionMatrix(holdout_prediction,as.factor(data_holdout$growth_f))$table
  cm[[as.character(thr)]] <- cm_thr
  # Categorize to true positive/false positive
  true_positive_rates <- c(true_positive_rates, cm_thr["growth", "growth"] /
                             (cm_thr["growth", "growth"] + cm_thr["no_growth", "growth"]))
  false_positive_rates <- c(false_positive_rates, cm_thr["growth", "no_growth"] /
                              (cm_thr["growth", "no_growth"] + cm_thr["no_growth", "no_growth"]))
}

tpr_fpr_for_thresholds <- tibble(
  "threshold" = thresholds,
  "true_positive_rate"  = true_positive_rates,
  "false_positive_rate" = false_positive_rates
)

ggplot(
  data = tpr_fpr_for_thresholds,
  aes(x = false_positive_rate, y = true_positive_rate)) +
  labs(x = "False positive rate (1 - Specificity)", y = "True positive rate (Sensitivity)") +
  geom_point(size=3, alpha=0.8, color= 'darkblue') +
  scale_x_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  theme_bw() +
  theme(legend.position ="right") +
  theme(legend.title = element_text(size = 4), 
        legend.text = element_text(size = 4),
        legend.key.size = unit(.4, "cm")) 



# autogenerate ROC curve using roc() function attributes
roc_obj_holdout <- roc(data_holdout$growth, data_holdout$best_logit_no_loss_pred, quiet = T)

roc_df <- cbind(roc_obj_holdout$sensitivities, roc_obj_holdout$specificities) %>% 
  as.data.frame() %>%
  mutate(FPR = 1 - V2)

roc_df <- roc_df %>% rename('TPR' = 'V1', 'TNR' = 'V2')

print(paste0('Model AUC is ', round(roc_obj_holdout$auc, 3), '.'))

ggplot(data = roc_df, aes(x = FPR, y = TPR)) +
  geom_line(color='darkblue', size = 1) +
  xlab("False Positive Rate (1-Specifity)") +
  ylab("True Positive Rate (Sensitivity)") +
  labs(title = paste0('Model X3 ROC with AUC of ', round(roc_obj_holdout$auc, 3))) + 
  geom_abline(intercept = 0, slope = 1,  linetype = "dashed", size = 1, col = "darkblue") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0, 0.01)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0.01, 0)) +
  theme_bw()


# 2.4 compare models based on average CV  RMSE and average AUC for 
#   3 logit models + lasso

CV_RMSE <- list()
CV_AUC <- list()

for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}

nvars <- lapply(logit_models, FUN = function(x) length(x$coefnames))
nvars[["LASSO"]] <- sum(lasso_coeffs != 0)

logit_summary_no_loss_function <- data.frame(
  "Number of predictors" = unlist(nvars),
  "CV RMSE" = unlist(CV_RMSE),
  "CV AUC" = unlist(CV_AUC))
```



```{r}
# STEP 3: USER-DEFINED LOSS FUNCTION
# relative cost of of a false negative classification (as compared with a false positive classification)

FP=1
FN=10
cost = FN/FP

# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))

prevelance = sum(data_train$growth)/length(data_train$growth)


# ROC curve help us find the optimal threshold with regard to the loss function

best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()

for (model_name in names(logit_models)) {
  
  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")
  
  best_tresholds_cv <- list()
  expected_loss_cv <- list()
  
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$growth, quiet = TRUE)
    # add the weights (costs) here!
    best_treshold <- coords(roc_obj, "best", 
                            ret="all", 
                            transpose = FALSE,
                            best.method="youden", 
                            best.weights=c(cost, prevelance))
    # save best threshold for each fold and save the expected loss value
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$growth)
  }

  # average thresholds and losses from each fold
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv))
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv))

  # at the end of the loop it will store data for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]

}

# note: no-loss-function Youden index is J = sensitivity + specificity - 1 = TPR + TNR - 1
# Youden with a loss function is: J = sensitivity + specificity - 1 = TPR + TNR * (1 - prevalence) / (cost * prevalence)
# we are looking for the threshold which produces the maximum J for our model

logit_summary_with_loss_function <- data.frame(
      "Avg of optimal thresholds" = unlist(best_tresholds),
      "Threshold for Fold5" = sapply(logit_cv_threshold, function(x) {x$threshold}),
      "Avg expected loss" = unlist(expected_loss),
      "Expected loss for Fold5" = unlist(logit_cv_expected_loss))

best_logit_with_loss <- logit_models[["X2"]]
best_logit_optimal_treshold <- best_tresholds[["X2"]]


# predict the probabilities on holdout
logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"growth"]


# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$growth, data_holdout[, "best_logit_with_loss_pred", drop=TRUE],quiet = TRUE)


# get expected loss on holdout:
holdout_treshold <- coords(
    roc_obj_holdout, 
    x = best_logit_optimal_treshold, 
    input= "threshold",
    ret="all", 
    transpose = FALSE)


# calculate the expected loss on holdout sample
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$growth)
expected_loss_holdout


# confusion table on holdout with optimal threshold
holdout_prediction <-
  ifelse(data_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "no_growth", "growth") %>%
  factor(levels = c("no_growth", "growth"))
cm_object3 <- confusionMatrix(holdout_prediction,as.factor(data_holdout$growth_f))

cm_3 = cm_object3$table
cm_3
```


```{r}
# RF model

train_control <- trainControl(
  method = "cv",
  n = 5,
  classProbs = TRUE, # same as probability = TRUE in ranger
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE,
  verboseIter = TRUE
)

tune_grid <- expand.grid(
  .mtry = c(5,6,7),
  .splitrule = "gini",
  .min.node.size = c(10,15) 
)

set.seed(13505)
rf_model_p <- train(
  formula(paste0("growth_f ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = data_train,
  tuneGrid = tune_grid,
  trControl = train_control
)

rf_model_p$results

best_mtry <- rf_model_p$bestTune$mtry
best_min_node_size <- rf_model_p$bestTune$min.node.size

# add metrics (RMSE, AUC) to metric containers

CV_RMSE_folds[["rf_p"]] <- rf_model_p$resample[,c("Resample", "RMSE")]

auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(Resample == fold)
  
  roc_obj <- roc(cv_fold$obs, cv_fold$growth, quiet = TRUE)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["rf_p"]] <- data.frame("Resample" = names(auc),
                                     "AUC" = unlist(auc))

CV_RMSE[["rf_p"]] <- mean(CV_RMSE_folds[["rf_p"]]$RMSE)
CV_AUC[["rf_p"]] <- mean(CV_AUC_folds[["rf_p"]]$AUC)

# use the loss function 

best_tresholds_cv <- list()
expected_loss_cv <- list()

for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(mtry == best_mtry,
           min.node.size == best_min_node_size,
           Resample == fold)
  
  roc_obj <- roc(cv_fold$obs, cv_fold$growth, quiet = TRUE)
  best_treshold <- coords(roc_obj, "best", 
                          ret="all", transpose = FALSE,
                          best.method="youden", 
                          best.weights=c(cost, prevelance))
  best_tresholds_cv[[fold]] <- best_treshold$threshold
  expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$growth)
}


# average loss over the folds & the last fold

best_tresholds[["rf_p"]] <- mean(unlist(best_tresholds_cv))
expected_loss[["rf_p"]] <- mean(unlist(expected_loss_cv))

rf_summary <- data.frame("CV RMSE" = CV_RMSE[["rf_p"]],
                         "CV AUC" = CV_AUC[["rf_p"]],
                         "Avg of optimal thresholds" = best_tresholds[["rf_p"]],
                         "Threshold for Fold5" = best_treshold$threshold,
                         "Avg expected loss" = expected_loss[["rf_p"]],
                         "Expected loss for Fold5" = expected_loss_cv[[fold]])

rf_summary


# plot loss and roc for fold5 

createRocPlotWithOptimal(roc_obj, best_treshold, "rf_p_roc_plot")


# estimate RMSE, AUC & loss on the holdout data

rf_predicted_probabilities_holdout <- predict(rf_model_p, newdata = data_holdout, type = "prob")
data_holdout$rf_p_prediction <- rf_predicted_probabilities_holdout[,"growth"]
RMSE(data_holdout$rf_p_prediction, data_holdout$growth)




# ROC curve on holdout

roc_obj_holdout <- roc(data_holdout$growth, data_holdout[, "rf_p_prediction", drop=TRUE], quiet=TRUE)


# AUC
as.numeric(roc_obj_holdout$auc)

# expected loss on holdout with optimal threshold

holdout_treshold <- coords(roc_obj_holdout, x = best_tresholds[["rf_p"]] , input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$growth)
expected_loss_holdout


# summary results

nvars[["rf_p"]] <- length(rfvars)

summary_results <- data.frame("Number of predictors" = unlist(nvars),
                              "CV RMSE" = unlist(CV_RMSE),
                              "CV AUC" = unlist(CV_AUC),
                              "CV threshold" = unlist(best_tresholds),
                              "CV expected Loss" = unlist(expected_loss))

model_names <- c("Logit X1", "Logit X2",
                 "Logit LASSO","RF probability")
summary_results <- summary_results %>%
  filter(rownames(.) %in% c("X1", "X2", "LASSO", "rf_p"))
rownames(summary_results) <- model_names

summary_results

# Classification with random forest
# split by Gini, majority for each obs over trees
# each observation is classified as 0 or 1 depending which value has a majority 
# in all of the trees
#
# USE ONLY when wins & losses are symmetric in the classes

train_control <- trainControl(
  method = "cv",
  n = 5,
  verboseIter = TRUE
)

set.seed(13505)
rf_model_f <- train(
  formula(paste0("growth_f ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = data_train,
  tuneGrid = tune_grid,
  trControl = train_control
)




# calculate expected loss by predicting on the holdout set using our loss function

data_holdout$rf_f_prediction_class <- predict(rf_model_f, newdata = data_holdout, type = "raw")

fp <- sum(data_holdout$rf_f_prediction_class == "growth"    & data_holdout$growth_f == "no_growth")
fn <- sum(data_holdout$rf_f_prediction_class == "no_growth" & data_holdout$growth_f == "growth")

(fp*FP + fn*FN)/length(data_holdout$growth)


# confusion matrix for random forest classification

cm_object4 <- confusionMatrix(data_holdout$rf_f_prediction_class,as.factor(data_holdout$growth_f))
cm_4 = cm_object4$table
cm_4
```





